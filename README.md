# Redpanda Iceberg Topics â†’ DuckDB â†’ Redpanda: Simplified Lakehouse Pipeline

> **A fully Dockerized streaming lakehouse pipeline** demonstrating Redpanda's native Iceberg integration with Polaris REST catalog and DuckDB analytics.

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)
[![Tested](https://img.shields.io/badge/e2e%20test-passing-brightgreen)](./validation/e2e.sh)
[![Docker](https://img.shields.io/badge/docker-required-blue)](https://www.docker.com/)
[![Redpanda](https://img.shields.io/badge/Redpanda-v25.2.2-red)](https://redpanda.com/)
[![DuckDB](https://img.shields.io/badge/DuckDB-v1.4.1-yellow)](https://duckdb.org/)

---

## ğŸ“Š Architecture Overview

### High-Level Ecosystem

```mermaid
graph TB
    subgraph "Data Ingestion Layer"
        P[Python Producer<br/>Trade Generator]
    end

    subgraph "Streaming Platform"
        RP[Redpanda Broker<br/>w/ Native Iceberg]
        T1[Topic: trades<br/>Iceberg Mode]
        T2[Topic: trade_analytics<br/>Iceberg Mode]
    end

    subgraph "Lakehouse Storage Layer"
        S3[MinIO S3<br/>Parquet Files]
        POL[Polaris REST Catalog<br/>Metadata Management]
        PG[(PostgreSQL<br/>Polaris DB)]
    end

    subgraph "Analytics Layer"
        DDB[DuckDB<br/>OLAP Engine]
    end

    subgraph "Monitoring"
        CON[Redpanda Console<br/>:8080]
        MCON[MinIO Console<br/>:9001]
    end

    P -->|10k trades| T1
    T1 -->|Auto Snapshot<br/>every 10s| RP
    RP -->|Write Parquet| S3
    RP -->|Register Metadata| POL
    POL -->|Store Metadata| PG
    DDB -->|REST Catalog API| POL
    DDB -->|Read Parquet| S3
    DDB -->|Publish Analytics| T2
    T2 -->|Auto Snapshot<br/>every 10s| RP

    CON -.->|Monitor| RP
    MCON -.->|Monitor| S3

    style RP fill:#ff6b6b
    style DDB fill:#ffd93d
    style POL fill:#6bcf7f
    style S3 fill:#4d96ff
```

### Bidirectional Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   COMPLETE BIDIRECTIONAL ICEBERG CYCLE                    â”‚
â”‚              Kafka â†’ Iceberg â†’ DuckDB â†’ Kafka â†’ Iceberg â†’ DuckDB         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Producer   â”‚  Generate deterministic trade data
    â”‚   (Python)  â”‚  â€¢ 10,000 trades
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â€¢ 8 stock symbols (AAPL, GOOGL, etc.)
           â”‚         â€¢ Fixed seed for reproducibility
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Redpanda Iceberg Topic  â”‚  Step 1: Ingest to Kafka
    â”‚     "trades"             â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  mode: value_schema      â”‚  Redpanda automatically:   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â€¢ Parses JSON schema      â”‚
           â”‚                      â€¢ Flattens to columns     â”‚
           â”‚                      â€¢ Converts to Parquet     â”‚
           â”‚ Auto Snapshot        â€¢ Uploads to S3           â”‚
           â”‚ (every 10s)          â€¢ Registers in Polaris    â”‚
           â–¼                                                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
    â”‚  Iceberg Table: trades      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚  â€¢ S3: Parquet files        â”‚  Step 2: Automatic Iceberg Table
    â”‚  â€¢ Polaris: Metadata        â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ DuckDB connects via REST API
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   DuckDB OLAP Engine     â”‚  Step 3: Query & Aggregate
    â”‚  â€¢ OAuth2 auth           â”‚  â€¢ GROUP BY symbol
    â”‚  â€¢ Read Parquet from S3  â”‚  â€¢ Compute avg/min/max price
    â”‚  â€¢ Aggregate analytics   â”‚  â€¢ Count BUY/SELL trades
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â€¢ Sum volumes
           â”‚
           â”‚ Publish results back to Redpanda
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Redpanda Iceberg Topic  â”‚  Step 4: Analytics â†’ Kafka
    â”‚   "trade_analytics"      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  mode: value_schema      â”‚  Redpanda automatically:   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â€¢ Creates Iceberg table   â”‚
           â”‚                      â€¢ 8 analytics records     â”‚
           â”‚ Auto Snapshot                                   â”‚
           â”‚ (every 10s)                                     â”‚
           â–¼                                                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
    â”‚  Iceberg Table: analytics   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚  â€¢ S3: Parquet files        â”‚  Step 5: Second Iceberg Table
    â”‚  â€¢ Polaris: Metadata        â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ DuckDB can query again!
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Query Analytics Table  â”‚  Step 6: Complete Cycle
    â”‚  FROM iceberg_catalog    â”‚  âœ“ Bidirectional flow proven
    â”‚   .redpanda.analytics    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Interaction Sequence

```mermaid
sequenceDiagram
    participant P as Producer
    participant R as Redpanda
    participant S as MinIO S3
    participant PL as Polaris
    participant DB as PostgreSQL
    participant D as DuckDB

    Note over P,D: Step 1-2: Ingest & Snapshot trades
    P->>R: Produce 10k trades (JSON)
    R->>R: Parse JSON schema
    R->>R: Convert to Parquet
    R->>S: Upload Parquet files
    R->>PL: Register Iceberg metadata
    PL->>DB: Store metadata

    Note over P,D: Step 3: Query trades Iceberg table
    D->>PL: OAuth2 authentication
    PL-->>D: Access token
    D->>PL: GET table metadata
    PL-->>D: Table location & schema
    D->>S: Read Parquet files
    S-->>D: Trade data
    D->>D: Aggregate (GROUP BY symbol)

    Note over P,D: Step 4-5: Publish & Snapshot analytics
    D->>R: Produce 8 analytics (JSON)
    R->>R: Parse analytics schema
    R->>R: Convert to Parquet
    R->>S: Upload analytics Parquet
    R->>PL: Register analytics metadata
    PL->>DB: Store analytics metadata

    Note over P,D: Step 6: Query analytics Iceberg table
    D->>PL: GET analytics metadata
    PL-->>D: Analytics table info
    D->>S: Read analytics Parquet
    S-->>D: Analytics data
    D->>D: âœ“ Bidirectional cycle complete
```

### Technology Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TECHNOLOGY LAYERS                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š ANALYTICS LAYER
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  DuckDB v1.4.1                       â”‚  OLAP Analytics Engine
    â”‚  â€¢ Iceberg extension                 â”‚  â€¢ Fast columnar queries
    â”‚  â€¢ httpfs extension (S3 access)      â”‚  â€¢ In-memory processing
    â”‚  â€¢ Python API (confluent-kafka)      â”‚  â€¢ REST catalog support
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸŒŠ STREAMING LAYER
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Redpanda v25.2.2                    â”‚  Kafka API Compatible
    â”‚  â€¢ Native Iceberg Topics             â”‚  â€¢ Zero-latency streaming
    â”‚  â€¢ Auto Parquet conversion           â”‚  â€¢ Schema Registry
    â”‚  â€¢ 10s snapshot interval             â”‚  â€¢ Cloud storage tiering
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“š CATALOG LAYER
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Apache Polaris (latest)             â”‚  Iceberg REST Catalog
    â”‚  â€¢ OAuth2 authentication             â”‚  â€¢ Open-source
    â”‚  â€¢ RBAC for tables/namespaces        â”‚  â€¢ Multi-catalog support
    â”‚  â€¢ PostgreSQL metadata store         â”‚  â€¢ Standard REST API
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¾ STORAGE LAYER
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  MinIO (S3 API)    â”‚  â”‚  PostgreSQL 15          â”‚
    â”‚  â€¢ Parquet files   â”‚  â”‚  â€¢ Polaris metadata     â”‚
    â”‚  â€¢ Path-style URLs â”‚  â”‚  â€¢ ACID transactions    â”‚
    â”‚  â€¢ Versioned       â”‚  â”‚  â€¢ High availability    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ APPLICATION LAYER
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Python Producer   â”‚  â”‚  Redpanda Console       â”‚
    â”‚  â€¢ Deterministic   â”‚  â”‚  â€¢ Topic monitoring     â”‚
    â”‚  â€¢ Seeded RNG      â”‚  â”‚  â€¢ Schema visualization â”‚
    â”‚  â€¢ JSON output     â”‚  â”‚  â€¢ Consumer lag         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ What This Demo Shows

This project demonstrates a **simplified lakehouse architecture** using Redpanda's native Iceberg integration:

### Architecture Simplification

```mermaid
graph LR
    subgraph "âŒ Traditional Approach (Complex)"
        K1[Kafka] --> SP[Spark Jobs]
        SP --> CS[Custom Committer]
        CS --> IC1[Iceberg]
        K1 --> CON[Connect/Benthos]
        CON --> S31[S3]
        CS --> S31
        IC1 --> TAB[Tabular Catalog]
        TAB --> META1[(Metadata)]
        SP -.->|Requires| CLUSTER[Spark Cluster]
        CON -.->|Config| YAML[Complex YAML]
    end

    subgraph "âœ… This Demo (Simplified)"
        RP[Redpanda<br/>Native Iceberg] --> IC2[Iceberg]
        IC2 --> POL[Polaris<br/>Open Source]
        POL --> META2[(PostgreSQL)]
        RP --> S32[MinIO S3]
        DDB[DuckDB] --> POL
        DDB --> S32
    end

    style K1 fill:#ffcccc
    style SP fill:#ffcccc
    style CS fill:#ffcccc
    style CON fill:#ffcccc
    style TAB fill:#ffcccc
    style RP fill:#ccffcc
    style IC2 fill:#ccffcc
    style POL fill:#ccffcc
    style DDB fill:#ccffcc
```

### Component Comparison

| Aspect | Traditional (7+ services) | This Demo (5 services) |
|--------|---------------------------|------------------------|
| **Iceberg Integration** | Custom Python committer service | âœ… Native Redpanda feature |
| **Data Movement** | Redpanda Connect + YAML config | âœ… Built-in (topic config) |
| **Processing Engine** | Apache Spark cluster | âœ… Not needed |
| **Catalog** | Tabular (proprietary) | âœ… Polaris (open-source) |
| **Snapshot Creation** | Manual polling + conversion | âœ… Automatic (every 10s) |
| **Complexity** | High (multiple moving parts) | âœ… Low (native integration) |
| **Setup Time** | 30-60 minutes | âœ… 5-10 minutes |

### Key Simplifications

âŒ **Removed Components:**
- No Spark cluster or jobs
- No custom committer service
- No Redpanda Connect/Benthos
- No complex YAML configurations
- No Tabular catalog subscription

âœ… **Simplified Stack:**
- **Redpanda** with native Iceberg Topics feature
- **Apache Polaris** - Open-source Iceberg REST catalog
- **PostgreSQL** - Polaris metadata storage
- **MinIO** - S3-compatible object storage
- **DuckDB** - Fast analytical queries via REST catalog

### Real-World Use Cases

```mermaid
graph TD
    subgraph "Financial Trading Platform"
        T1[Trade Execution System] -->|Real-time trades| RP1[Redpanda Iceberg Topic]
        RP1 --> IC1[Iceberg Table: trades]
        IC1 --> AN1[DuckDB Analytics]
        AN1 -->|Risk metrics| RP2[Analytics Topic]
        RP2 --> DASH1[Risk Dashboard]
        RP2 --> ALERT1[Alert System]
    end

    subgraph "IoT Sensor Network"
        IOT[IoT Devices] -->|Sensor readings| RP3[Redpanda Iceberg Topic]
        RP3 --> IC2[Iceberg Table: sensors]
        IC2 --> AN2[DuckDB Analytics]
        AN2 -->|Anomaly detection| RP4[Alerts Topic]
        RP4 --> ML[ML Pipeline]
        RP4 --> OPS[Operations Console]
    end

    subgraph "E-commerce Platform"
        WEB[Web Traffic] -->|Clickstream| RP5[Redpanda Iceberg Topic]
        RP5 --> IC3[Iceberg Table: events]
        IC3 --> AN3[DuckDB Analytics]
        AN3 -->|User segments| RP6[Segments Topic]
        RP6 --> REC[Recommendation Engine]
        RP6 --> BI[BI Dashboard]
    end

    style RP1 fill:#ff6b6b
    style RP3 fill:#ff6b6b
    style RP5 fill:#ff6b6b
    style AN1 fill:#ffd93d
    style AN2 fill:#ffd93d
    style AN3 fill:#ffd93d
```

**This Demo Implements: Financial Trading Platform Pattern**
- Ingest raw financial trade data
- Redpanda automatically creates Iceberg tables
- Query with DuckDB via REST catalog
- Publish insights for real-time monitoring dashboards, alerting systems, and downstream microservices

---

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose installed
- 8GB RAM minimum
- ~5-10 minutes for full end-to-end test

### Run the Complete Pipeline

```bash
# Clone and navigate
cd redpanda-iceberg-duckdb

# Run end-to-end test (builds containers, starts services, runs full workflow)
./validation/e2e.sh
```

**What the script does:**
1. âœ… Starts all services (Redpanda, Polaris, MinIO, PostgreSQL, DuckDB)
2. âœ… Creates TWO Redpanda topics with `redpanda.iceberg.mode=value_schema_latest`
3. âœ… Registers JSON schemas for both trade and analytics data
4. âœ… Produces 10,000 deterministic trade records
5. âœ… Waits for Redpanda to create `trades` Iceberg table (automatic)
6. âœ… Queries `trades` Iceberg table with DuckDB via Polaris REST catalog
7. âœ… Publishes aggregated results to `trade_analytics` topic (also Iceberg-enabled)
8. âœ… Waits for Redpanda to create `trade_analytics` Iceberg table
9. âœ… Queries `trade_analytics` Iceberg table (completing bidirectional cycle!)

**Expected Output:**
```
=== End-to-End Test PASSED ===
âœ“ Data successfully flowed through BIDIRECTIONAL Iceberg pipeline:
  1. Producer â†’ Redpanda (trades topic with Iceberg mode)
  2. Redpanda â†’ trades Iceberg table (automatic snapshots)
  3. DuckDB â†’ Query trades Iceberg table via REST catalog
  4. DuckDB â†’ Publish analytics to Redpanda (trade_analytics topic with Iceberg mode)
  5. Redpanda â†’ trade_analytics Iceberg table (automatic snapshots)
  6. DuckDB â†’ Query analytics Iceberg table (complete cycle!)

Bidirectional Iceberg Flow Demonstrated:
  âœ“ Kafka â†’ Iceberg â†’ DuckDB â†’ Kafka â†’ Iceberg â†’ DuckDB
  âœ“ Both directions use native Redpanda Iceberg Topics
  âœ“ All data queryable via Polaris REST catalog
```

---

## ğŸ“‹ Manual Step-by-Step

### 1. Start Services

```bash
# Build DuckDB container
docker compose build duckdb-cli

# Start all services
docker compose up -d

# Wait for services to be ready (~20 seconds)
docker compose ps
```

### 2. Create Topic with Iceberg Mode

```bash
# Create topic with Iceberg integration
docker compose exec redpanda rpk topic create trades \
    --partitions 1 \
    --replicas 1 \
    --topic-config=redpanda.iceberg.mode=value_schema_latest

# Register JSON schema
docker compose exec redpanda rpk registry schema create trades-value \
    --schema @/path/to/schema.json \
    --type json
```

### 3. Produce Data

```bash
# Produce 10,000 trades
docker compose run --rm producer \
    --brokers redpanda:9092 \
    --topic trades \
    --count 10000 \
    --seed 42
```

### 4. Wait for Iceberg Snapshot

```bash
# Redpanda creates snapshots automatically every 10 seconds
sleep 15
```

### 5. Query with DuckDB

```bash
# Generate DuckDB init script with OAuth2 token
docker compose exec duckdb-cli /workspace/duckdb/get_token.sh

# Query Iceberg table via REST catalog
docker compose exec duckdb-cli duckdb -init /workspace/duckdb/init-env.sql -c "
SELECT symbol, COUNT(*) as trades, AVG(price) as avg_price
FROM iceberg_catalog.redpanda.trades
GROUP BY symbol;
"
```

### 6. Publish Analytics

```bash
# Query and publish to Redpanda
docker compose exec \
    -e RP_BROKERS=redpanda:9092 \
    -e RP_TOPIC_RESULTS=trade_analytics \
    duckdb-cli python3 /workspace/duckdb/query_and_publish.py

# Verify results
docker compose exec redpanda rpk topic consume trade_analytics --num 8 --format json
```

---

## ğŸ”§ Configuration

### Redpanda Iceberg Settings

Key cluster configurations (set via `--set` flags in docker-compose.yml):

```yaml
iceberg_enabled: true
iceberg_catalog_type: rest
iceberg_rest_catalog_endpoint: http://polaris:8181/api/catalog/
iceberg_rest_catalog_oauth2_server_uri: http://polaris:8181/api/catalog/v1/oauth/tokens
iceberg_rest_catalog_authentication_mode: oauth2
iceberg_rest_catalog_client_id: root
iceberg_rest_catalog_client_secret: pass
iceberg_rest_catalog_warehouse: redpanda_catalog
iceberg_target_lag_ms: 10000  # Snapshot every 10 seconds
```

### Iceberg Modes

Redpanda supports three Iceberg modes via `redpanda.iceberg.mode`:

1. **`key_value`** - Raw bytes stored as BLOB columns
2. **`value_schema_id_prefix`** - Avro encoded with schema ID prefix
3. **`value_schema_latest`** - JSON encoded with latest schema (used in this demo)

### DuckDB REST Catalog Connection

```sql
-- Create secret with OAuth2 token
CREATE SECRET iceberg_secret (
    TYPE ICEBERG,
    TOKEN '<polaris-oauth2-token>'
);

-- Attach Polaris catalog
ATTACH 'redpanda_catalog' AS iceberg_catalog (
    TYPE ICEBERG,
    SECRET iceberg_secret,
    ENDPOINT 'http://polaris:8181/api/catalog/'
);

-- Query tables
SELECT * FROM iceberg_catalog.redpanda.trades;
```

---

## ğŸ“ Project Structure

```
redpanda-iceberg-duckdb/
â”œâ”€â”€ docker-compose.yml              # Services orchestration
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ CLAUDE.md                       # Developer guide
â”‚
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ create-polaris-db.sql       # PostgreSQL initialization
â”‚
â”œâ”€â”€ validation/
â”‚   â””â”€â”€ e2e.sh                      # End-to-end test script
â”‚
â”œâ”€â”€ redpanda/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ producer.py                 # Trade data generator
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ duckdb/
â”‚   â”œâ”€â”€ init.sql                    # DuckDB REST catalog setup
â”‚   â”œâ”€â”€ get_token.sh                # OAuth2 token helper
â”‚   â”œâ”€â”€ verify_iceberg.sql          # Verification queries
â”‚   â””â”€â”€ query_and_publish.py        # Query + publish to Kafka
â”‚
â””â”€â”€ duckdb-ui/
    â””â”€â”€ Dockerfile                  # DuckDB CLI with Python
```

---

## ğŸ“Š Data Schema

### Trade Data Schema (Input)

```mermaid
classDiagram
    class Trade {
        +String trade_id
        +String symbol
        +Double price
        +Integer qty
        +String side
        +Timestamp ts_event
        +String notes
        --
        Constraints
        trade_id: UUID v4
        symbol: AAPL|GOOGL|MSFT|AMZN|TSLA|NVDA|META|NFLX
        price: $50.00 - $499.95
        qty: 1 - 1000 shares
        side: BUY or SELL
        ts_event: ISO 8601 with microsecond precision
    }
```

**JSON Schema (Registered in Schema Registry):**
```json
{
  "type": "object",
  "properties": {
    "trade_id": {"type": "string"},
    "symbol": {"type": "string"},
    "price": {"type": "number"},
    "qty": {"type": "integer"},
    "side": {"type": "string"},
    "ts_event": {"type": "string", "format": "date-time"},
    "notes": {"type": "string"}
  },
  "required": ["trade_id", "symbol", "price", "qty", "side", "ts_event"]
}
```

**Iceberg Table Columns (Auto-created by Redpanda):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Column       â”‚ Iceberg Type    â”‚ Description                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trade_id     â”‚ string          â”‚ UUID v4 identifier          â”‚
â”‚ symbol       â”‚ string          â”‚ Stock ticker (8 symbols)    â”‚
â”‚ price        â”‚ double          â”‚ Trade price                 â”‚
â”‚ qty          â”‚ int             â”‚ Quantity (shares)           â”‚
â”‚ side         â”‚ string          â”‚ BUY or SELL                 â”‚
â”‚ ts_event     â”‚ timestamptz     â”‚ Event timestamp (Î¼s)        â”‚
â”‚ notes        â”‚ string          â”‚ Optional metadata           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Analytics Data Schema (Output)

```mermaid
classDiagram
    class TradeAnalytics {
        +String symbol
        +Integer trade_count
        +Double avg_price
        +Double min_price
        +Double max_price
        +Integer total_volume
        +Integer buy_count
        +Integer sell_count
        +String first_trade_time
        +String last_trade_time
        --
        Computed by DuckDB
        Aggregated per symbol
        Published to trade_analytics topic
        Also becomes Iceberg table
    }
```

### Docker Networking

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Docker Network: lakehouse                     â”‚
â”‚                         (Bridge Mode)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Internal DNS Resolution (all services resolve by name)

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  redpanda   â”‚â—„â”€â”€â”€â”€â–ºâ”‚   polaris    â”‚â—„â”€â”€â”€â”€â–ºâ”‚  postgres   â”‚
    â”‚   :9092     â”‚      â”‚   :8181      â”‚      â”‚   :5432     â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                    â”‚
           â”‚                    â”‚
           â–¼                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   minio     â”‚      â”‚  duckdb-cli  â”‚      â”‚   console   â”‚
    â”‚   :9000     â”‚      â”‚   (Python)   â”‚      â”‚   :8080     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Port Mappings (host:container)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  â€¢ localhost:19092 â†’ redpanda:9092  (Kafka API)       â”‚
    â”‚  â€¢ localhost:18081 â†’ redpanda:8081  (Schema Registry) â”‚
    â”‚  â€¢ localhost:8181  â†’ polaris:8181   (REST Catalog)    â”‚
    â”‚  â€¢ localhost:9000  â†’ minio:9000     (S3 API)          â”‚
    â”‚  â€¢ localhost:9001  â†’ minio:9001     (MinIO Console)   â”‚
    â”‚  â€¢ localhost:8080  â†’ console:8080   (Redpanda UI)     â”‚
    â”‚  â€¢ localhost:5432  â†’ postgres:5432  (PostgreSQL)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Monitoring & Debugging

### Access Web UIs

- **Redpanda Console:** http://localhost:8080
- **MinIO Console:** http://localhost:9001 (login: `minioadmin` / `minioadmin`)
- **Polaris API:** http://localhost:8181

### Check Service Logs

```bash
# Redpanda logs (shows Iceberg snapshot creation)
docker compose logs -f redpanda

# Polaris logs
docker compose logs -f polaris

# DuckDB container
docker compose logs -f duckdb-cli
```

### Query Polaris Catalog

```bash
# Get OAuth2 token
TOKEN=$(curl -s -X POST http://localhost:8181/api/catalog/v1/oauth/tokens \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "grant_type=client_credentials&client_id=root&client_secret=pass&scope=PRINCIPAL_ROLE:ALL" \
  | jq -r '.access_token')

# List catalogs
curl -s http://localhost:8181/api/management/v1/catalogs \
  -H "Authorization: Bearer $TOKEN" | jq

# List namespaces in catalog
curl -s http://localhost:8181/api/catalog/v1/redpanda_catalog/namespaces \
  -H "Authorization: Bearer $TOKEN" | jq

# List tables
curl -s http://localhost:8181/api/catalog/v1/redpanda_catalog/namespaces/redpanda/tables \
  -H "Authorization: Bearer $TOKEN" | jq
```

---

## ğŸ› Troubleshooting

### Token Expired

If DuckDB queries fail with "token expired":

```bash
docker compose exec duckdb-cli /workspace/duckdb/get_token.sh
```

### No Data in Iceberg Table

Check that Iceberg snapshots are being created:

```bash
# Check Redpanda logs for "iceberg" messages
docker compose logs redpanda | grep -i iceberg

# Verify files in MinIO
docker compose exec -it minio-init mc ls myminio/redpanda --recursive
```

### Polaris Connection Issues

```bash
# Check Polaris is healthy
curl http://localhost:8181/healthcheck

# Check PostgreSQL is running
docker compose exec postgres psql -U polaris -d polaris -c "\dt"
```

---

## ğŸ§¹ Clean Up

```bash
# Stop all services and remove volumes
docker compose down -v

# Remove generated files
rm -f duckdb/init-env.sql
```

---

## âœ¨ Key Features

### Core Capabilities

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FEATURE HIGHLIGHTS                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”„ BIDIRECTIONAL ICEBERG FLOW
    â”œâ”€ Kafka â†’ Iceberg â†’ DuckDB (forward path)
    â””â”€ DuckDB â†’ Kafka â†’ Iceberg (return path)
    âœ“ Complete round-trip demonstrated

âš¡ NATIVE ICEBERG INTEGRATION
    â”œâ”€ No custom services required
    â”œâ”€ Redpanda handles conversion automatically
    â”œâ”€ JSON â†’ Parquet in real-time
    â””â”€ Automatic metadata registration
    âœ“ Zero manual intervention

ğŸ“Š DUAL ICEBERG TABLES
    â”œâ”€ Table 1: trades (10,000 records)
    â””â”€ Table 2: trade_analytics (8 aggregates)
    âœ“ Both queryable via REST catalog

ğŸ¯ STANDARD REST CATALOG
    â”œâ”€ Apache Polaris (open-source)
    â”œâ”€ OAuth2 authentication
    â”œâ”€ RBAC for access control
    â””â”€ PostgreSQL metadata storage
    âœ“ Production-ready catalog

â±ï¸ AUTOMATIC SNAPSHOTS
    â”œâ”€ Configurable interval (10s default)
    â”œâ”€ No manual triggering needed
    â”œâ”€ Atomic commits to Iceberg
    â””â”€ ACID guarantees maintained
    âœ“ Set-and-forget operation

ğŸš€ DUCKDB ANALYTICS
    â”œâ”€ Sub-second query performance
    â”œâ”€ Direct Parquet reads from S3
    â”œâ”€ In-memory aggregations
    â””â”€ Kafka producer integration
    âœ“ Fast analytical processing

ğŸ§ª DETERMINISTIC TESTING
    â”œâ”€ Fixed seed (42) for reproducibility
    â”œâ”€ Same data every run
    â”œâ”€ End-to-end verification
    â””â”€ Automated test suite
    âœ“ Reliable CI/CD integration

ğŸ³ FULLY DOCKERIZED
    â”œâ”€ No local installation needed
    â”œâ”€ Self-contained environment
    â”œâ”€ Health checks for all services
    â””â”€ One command startup
    âœ“ Works anywhere Docker runs
```

### E2E Test Flow Timeline

```mermaid
gantt
    title End-to-End Test Execution Timeline (~5-10 minutes)
    dateFormat  mm:ss
    axisFormat  %M:%S

    section Setup
    Start services           :00:00, 00:20
    Health checks            :00:20, 00:10
    Polaris bootstrap        :00:30, 00:15
    Create topics            :00:45, 00:05

    section Ingestion
    Produce 10k trades       :00:50, 00:15
    Wait for snapshot        :01:05, 00:15

    section Query 1
    Get OAuth2 token         :01:20, 00:02
    Query trades table       :01:22, 00:05
    Aggregate analytics      :01:27, 00:03

    section Publish
    Publish to analytics     :01:30, 00:05
    Wait for snapshot        :01:35, 00:15

    section Query 2
    Query analytics table    :01:50, 00:03
    Verify bidirectional     :01:53, 00:02

    section Validation
    Check all 8 symbols      :01:55, 00:05
    âœ“ Test Complete          :milestone, 02:00, 0m
```

---

## ğŸ“ Learn More

### Documentation

- **[Redpanda Iceberg Topics](https://docs.redpanda.com/current/manage/iceberg/about-iceberg-topics/)** - Native Iceberg integration
- **[Apache Polaris](https://polaris.apache.org/)** - Open-source Iceberg REST catalog
- **[DuckDB Iceberg Extension](https://duckdb.org/docs/stable/core_extensions/iceberg/iceberg_rest_catalogs)** - REST catalog support
- **[Apache Iceberg](https://iceberg.apache.org/)** - Table format specification

### Related Resources

- [Redpanda with Snowflake Polaris](https://docs.redpanda.com/current/manage/iceberg/redpanda-topics-iceberg-snowflake-catalog/)
- [Reference Implementation](https://github.com/pmw-rp/iceberg-demo) - Kubernetes-based demo

---

## ğŸ¤ Contributing

This is a demo project for educational purposes. Feel free to:
- Open issues for bugs or questions
- Submit PRs for improvements
- Use as a template for your own projects

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

**Built with Claude Code** - Demonstrating modern lakehouse architecture with Redpanda's native Iceberg integration.

Special thanks to the teams behind Redpanda, Apache Iceberg, Apache Polaris, and DuckDB.

---

<p align="center">
  <b>â­ If you find this useful, please star the repository! â­</b>
</p>
